{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "86v_UyIPML02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "import pickle\n",
        "import librosa\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import ConvLSTM2D\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn8Dub7OMRBj",
        "colab_type": "code",
        "outputId": "45d4ffdc-e594-4925-d9b0-08dd5c89fc5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWm7oiMZMyuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_off=gzip.open(\"/content/drive/My Drive/COMP562/Obxd_onenote.data\",\"rb\")\n",
        "emp=pickle.load(pickle_off)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sJQPs14NEgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampling_rate=44100\n",
        "nsamples=1000;\n",
        "ntrain=600;\n",
        "nmel = 128\n",
        "nfeatures=66*(nmel)\n",
        "nparameters = 80\n",
        "\n",
        "X=np.zeros((nsamples,nmel,66))\n",
        "y=np.zeros((nsamples,nparameters))\n",
        "for i in range(nsamples):\n",
        "    specTemp=librosa.feature.melspectrogram(np.array(emp[i][0]), sr=sampling_rate, hop_length=1024)\n",
        "    X[i,:]=(specTemp)\n",
        "    y[i,:]=([emp[i][1][j][1] for j in range(0,nparameters)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWf6IQg5PftU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainIndex=set(np.random.choice(1000,size=ntrain,replace=False))\n",
        "testIndex=set(np.linspace(0,999,1000))-trainIndex\n",
        "x_train=X[np.array(list(trainIndex),dtype=int),:]\n",
        "y_train=y[np.array(list(trainIndex),dtype=int),:]\n",
        "x_test=X[np.array(list(testIndex),dtype=int),:]\n",
        "y_test=y[np.array(list(testIndex),dtype=int),:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACO-HmKxTpuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train=np.reshape(x_train,(ntrain,nmel,66,1))\n",
        "x_test=np.reshape(x_test,(nsamples-ntrain,nmel,66,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqpQdIeyncuK",
        "colab_type": "code",
        "outputId": "b6bc455f-f1f4-40e6-e353-8d3b47950b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3451
        }
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(64, (10, 4 ), activation='relu', input_shape=(nmel,66,1),data_format=\"channels_last\"))\n",
        "model.add(Conv2D(64, (4, 4), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 4), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 4), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(nparameters, activation='relu'))\n",
        "model.add(Dense(nparameters, activation='tanh'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse')\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=100,\n",
        "          batch_size=20)\n",
        "score = model.evaluate(x_test, y_test, batch_size=100)\n",
        "print(np.sqrt(score))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2967\n",
            "Epoch 2/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0994\n",
            "Epoch 3/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0877\n",
            "Epoch 4/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0873\n",
            "Epoch 5/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0879\n",
            "Epoch 6/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0872\n",
            "Epoch 7/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0863\n",
            "Epoch 8/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0864\n",
            "Epoch 9/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0867\n",
            "Epoch 10/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0854\n",
            "Epoch 11/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0856\n",
            "Epoch 12/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0852\n",
            "Epoch 13/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0849\n",
            "Epoch 14/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0846\n",
            "Epoch 15/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0843\n",
            "Epoch 16/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0842\n",
            "Epoch 17/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0839\n",
            "Epoch 18/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0840\n",
            "Epoch 19/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0837\n",
            "Epoch 20/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0838\n",
            "Epoch 21/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0833\n",
            "Epoch 22/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0837\n",
            "Epoch 23/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0833\n",
            "Epoch 24/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0834\n",
            "Epoch 25/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0835\n",
            "Epoch 26/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0836\n",
            "Epoch 27/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0833\n",
            "Epoch 28/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0835\n",
            "Epoch 29/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0832\n",
            "Epoch 30/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0831\n",
            "Epoch 31/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0829\n",
            "Epoch 32/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0833\n",
            "Epoch 33/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0830\n",
            "Epoch 34/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0831\n",
            "Epoch 35/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0829\n",
            "Epoch 36/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0830\n",
            "Epoch 37/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0828\n",
            "Epoch 38/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0830\n",
            "Epoch 39/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0828\n",
            "Epoch 40/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0834\n",
            "Epoch 41/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0828\n",
            "Epoch 42/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0826\n",
            "Epoch 43/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0828\n",
            "Epoch 44/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0824\n",
            "Epoch 45/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0825\n",
            "Epoch 46/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0825\n",
            "Epoch 47/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0821\n",
            "Epoch 48/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0817\n",
            "Epoch 49/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0821\n",
            "Epoch 50/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0815\n",
            "Epoch 51/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0818\n",
            "Epoch 52/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0814\n",
            "Epoch 53/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0812\n",
            "Epoch 54/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0812\n",
            "Epoch 55/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0806\n",
            "Epoch 56/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0808\n",
            "Epoch 57/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0802\n",
            "Epoch 58/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0803\n",
            "Epoch 59/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0802\n",
            "Epoch 60/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0794\n",
            "Epoch 61/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0796\n",
            "Epoch 62/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0791\n",
            "Epoch 63/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0786\n",
            "Epoch 64/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0786\n",
            "Epoch 65/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0783\n",
            "Epoch 66/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0781\n",
            "Epoch 67/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0776\n",
            "Epoch 68/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0776\n",
            "Epoch 69/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0771\n",
            "Epoch 70/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0769\n",
            "Epoch 71/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0767\n",
            "Epoch 72/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0766\n",
            "Epoch 73/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0763\n",
            "Epoch 74/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0759\n",
            "Epoch 75/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0757\n",
            "Epoch 76/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0751\n",
            "Epoch 77/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0754\n",
            "Epoch 78/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0752\n",
            "Epoch 79/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0746\n",
            "Epoch 80/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0747\n",
            "Epoch 81/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0745\n",
            "Epoch 82/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0739\n",
            "Epoch 83/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0738\n",
            "Epoch 84/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0739\n",
            "Epoch 85/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0732\n",
            "Epoch 86/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0736\n",
            "Epoch 87/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0730\n",
            "Epoch 88/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0728\n",
            "Epoch 89/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0726\n",
            "Epoch 90/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0725\n",
            "Epoch 91/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0722\n",
            "Epoch 92/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0720\n",
            "Epoch 93/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0718\n",
            "Epoch 94/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0717\n",
            "Epoch 95/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0714\n",
            "Epoch 96/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0716\n",
            "Epoch 97/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0710\n",
            "Epoch 98/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0710\n",
            "Epoch 99/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0710\n",
            "Epoch 100/100\n",
            "600/600 [==============================] - 1s 1ms/step - loss: 0.0707\n",
            "400/400 [==============================] - 0s 1ms/step\n",
            "0.3013193716805823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NEN_Ub0nngt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle_off=gzip.open(\"/content/drive/My Drive/COMP562/NSynth.data\",\"rb\")\n",
        "nsynth = pickle.load(pickle_off, encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbUsQsTqrAd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nsynth_processed=np.zeros((len(nsynth),nmel,66))\n",
        "for i in range(len(nsynth)):\n",
        "    specTemp=librosa.feature.melspectrogram(np.append(nsynth[i],np.zeros(1000)), sr=sampling_rate, hop_length=1024)\n",
        "    nsynth_processed[i,:]=(specTemp)\n",
        "nsynth_processed=np.reshape(nsynth_processed,(len(nsynth),nmel,66,1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj91bAOeq1vt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model.predict(nsynth_processed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYpot2xlsuFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datafile = gzip.open(\"/content/drive/My Drive/COMP562/Obxd_CNN_NSynth.data\",\"wb\")\n",
        "pickle.dump(results, datafile)\n",
        "datafile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOfmvs3QgjVB",
        "colab_type": "text"
      },
      "source": [
        "# Additional Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTU-EUb0QwtZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(64, (4, 4), activation='relu', input_shape=(nmel,66,1),data_format=\"channels_last\"))\n",
        "model.add(Conv2D(64, (4, 4), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(nparameters, activation='relu'))\n",
        "model.add(Dense(nparameters, activation='tanh'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse')\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=200,\n",
        "          batch_size=20)\n",
        "score = model.evaluate(x_test, y_test, batch_size=100)\n",
        "print(np.sqrt(score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXXRB5kWz5eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Sequential()\n",
        "model.add(Conv2D(128, (24, 4), activation='relu', input_shape=(nmel,66,1),data_format=\"channels_last\"))\n",
        "model.add(Conv2D(128, (4, 4), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 4), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 4), activation='sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(nparameters, activation='relu'))\n",
        "model.add(Dense(nparameters, activation='tanh'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='mse')\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=10,\n",
        "          batch_size=20)\n",
        "score = model.evaluate(x_test, y_test, batch_size=100)\n",
        "print(np.sqrt(score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}